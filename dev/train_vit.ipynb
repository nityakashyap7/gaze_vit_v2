{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faa3aaa",
   "metadata": {},
   "source": [
    "# Developing the training loop\n",
    "\n",
    "now that i have a util function to patch gaze masks and a hook class to extract the attention logits, i need to bring them into the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a32f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # allows me to pull from GABRIL_utils/ which is a sibling directory to dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.hook import AttentionExtractor\n",
    "from GABRIL_utils.utils import load_dataset\n",
    "from utils.patch_gaze_masks import patch_gaze_masks\n",
    "import torch.nn.functional as F\n",
    "import train_vit\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9561ef",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "## gaze predictions (model's QK_t logits)\n",
    "- shape: `(batch_size, num_heads, num_patches)`\n",
    "- raw logits, not softmaxed- this is ok bc thats what `F.cross_entropy()` wants\n",
    "\n",
    "## gaze targets (human gaze masks)\n",
    "- shape: `(batch_size, num_frames, num_patches)`\n",
    "- already softmaxed across `num_patches`\n",
    "\n",
    "`gaze_targs` needs to be broadcasted across the `num_heads` dim and the `num_frames`/`num_channels` dim needs to disappear. it shouldnt be the responsibility of the loss function to do the reshaping. loss function should be as dumb as possible, leaving the processing steps to train_step.\n",
    "\n",
    "\n",
    "So assume for a dumb loss function that u magically get `(batch_size, num_heads, num_patches)` for both `gaze_preds` and `gaze_targs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f990ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ce_only(self, action_preds, action_targs, **kwargs):\n",
    "        return F.cross_entropy(action_preds, action_targs)\n",
    "\n",
    "def _ce_plus_gaze_reg(self, action_preds, action_targs, gaze_preds, gaze_targs, reg_lambda=1.0, **kwargs):\n",
    "        ''' averages over batch_size and num_heads in one go. if u wanna see the intermediary ce values for each head separately u need to define a separate loss function'''\n",
    "        ce = self._ce_only(action_preds, action_targs)\n",
    "\n",
    "        #  collapse the batch_size and num_heads dim into one that cross_entropy will average over to return u a scalar (this averages across the batch and heads so u dont need to do 2 separate averages)\n",
    "        gaze_preds = rearrange(gaze_preds, 'b h n -> (b h) n')\n",
    "        gaze_targs = rearrange(gaze_targs, 'b h n -> (b h) n')\n",
    "        reg = reg_lambda * F.cross_entropy(gaze_preds, gaze_targs)\n",
    "\n",
    "        return ce + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09ef8c",
   "metadata": {},
   "source": [
    "note that `_ce_plus_gaze_reg()` collapses the `batch_size` and `num_heads` dims into one. This is because `F.cross_entropy()` anyway averages across the batch examples to return u 1 scalar value instead of 1 cross entropy value for each example in the batch. Then if u regularized each head in some sort of for loop, you'd get num_heads total cross entropy values. this wont do bc backprop needs a single scalar value to do gradient descent on. the way i choose to tackle this is to have the cross entropy values for each head also averaged. This is two average computations that i can combine into one if i just collapse the `batch_size` and `num_heads` dims before i pass into `F.cross_entropy`, which will average across the first dimension. \n",
    "\n",
    "This approach still regularizes each head separately, which is what yutai wanted.\n",
    "\n",
    "another option is to also mean pool the attention masks from each head before passing into `F.cross_entropy` but i think that only constrains the average mask (a head's attention map can drift if the other head(s) compensate). I don't think this would equate to regularizing each of the heads separately. but we'll test out both anyways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d6aad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaze-vit-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
